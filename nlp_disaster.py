# -*- coding: utf-8 -*-
"""NLP : Disaster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17aYP3R_hAeTukafNQmpcM86HUeXbfpgl

Problem Statement:

In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.

NLP Pipeline: Preprocessing | Feature Engineering | Modelling

1) **Preprocessing** : convert the text into lower case , tokensize the text , Stemming / lemitization , Noise removal (Punctuations) Stopword , POS , Tagging , NER Tag.

2) **Feature Engineering** - Bag of words / count vectorizer , TF-IDF , Word Embeddings

At this stage , we convert all the text into numbers and dataframe may get converted into Array
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
train.head()

train.keyword.nunique()

# Combining the data

combined = pd.concat([train,test],ignore_index = True)
combined.head()

combined['text_len'] = combined.text.str.len()

combined.loc[combined.target == 0, "text_len"].plot(kind = 'hist',alpha = 0.5, color = 'blue',label = 'Not Disaster')

combined.loc[combined.target == 1, "text_len"].plot(kind = 'hist',alpha = 0.5, color = 'red',label = 'Disaster')

plt.legend()
plt.show()

# Keywords

plt.figure(figsize = (8,72),dpi = 200)

combined["target_mean"] = combined.groupby("keyword")["target"].transform("mean")

input_x = combined.sort_values(by = "target_mean",ascending = False)["keyword"]
hue = combined.sort_values(by = "target_mean",ascending = False)["target"]

sns.countplot(y = input_x,hue = hue,data= combined)
plt.tick_params(axis = "y",size = 15)
plt.tick_params(axis = "x",size = 15)
plt.legend(loc = 1)
plt.show()

# Clean the text

import nltk
nltk.download("punkt")
nltk.download("stopwords")

import string
noise = string.punctuation

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,RegexpTokenizer
import re

# Stopwords
stop_words = stopwords.words("english")

print(stop_words)

text = "@nxwestmidlands huge fire at Wholesale markets ablaze http://t.co/rwzbFVNXER"
newtext = re.sub("@[a-zA-Z]+|http://[a-zA-Z]+.+"," ", text).strip()

regex.tokenize(newtext)

regex = RegexpTokenizer('[a-zA-Z]+')
regex.tokenize(text)
def clean_text(text):
    text = text.lower()
    text = re.sub("(www.+)|(\s+)|(@[a-zA-Z]+)|\W+", " ", text) # removes hyperlinks, special chars
    text = re.sub('(\w+:/\S+)', " ", text)
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub("http|https", " ", text)
    text = re.sub("[^a-zA-Z]+", " ", text)
    text = regex.tokenize(text)
    text = [word for word in text if len(word) > 3]
    text = [word for word in text if word not in stop_words]
    text = [word for word in text if word not in noise]
    return(text)

combined["text_clean"] = combined["text"].apply(clean_text)
combined.head()

# Visulaizing the Tokens

all_tokens = [token for review_tokens in combined.text_clean for token in review_tokens
              if token.lower()not in stop_words]

# Finding the Frequency of the tokens

from nltk.probability import FreqDist
freq_dist= FreqDist(all_tokens).items()

# Since we have created freq dist, let's create a DataFrame
df = pd.DataFrame(freq_dist,columns = ["Tokens","Frequency"]).sort_values(by = "Frequency",ascending = False)

# Visulazing the Top20 Tokens
plt.figure(figsize = (10,10))
plt.bar(df.Tokens[0:20],df.Frequency[0:20],color = "red")
plt.xticks(rotation = 90)
plt.title("Top 20 Tokens: Disaster Data")
plt.show()

# lets generate the word cloud too...

# pip install wordcloud
from wordcloud import WordCloud


# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_tokens))
plt.figure(figsize=(12, 6))

# In image processing, interpolation refers to the method used to estimate the values of pixels at non-integer coordinates.
# When displaying an image, interpolation determines how intermediate pixels are calculated
# when resizing or transforming the image.

plt.imshow(wordcloud, interpolation='bilinear')

plt.title('Word Cloud of Reviews (excluding stopwords)')
plt.axis('off')
plt.show()

# Display the DataFrame
print(df.head(20))

"""# Word Embeddings

Cosign Similarity

* Cosign angle b/w two vectors
* Value is b/w -1 to 1
* -ve 1 represents that there's a lot of dissimilarity b/w two vectors
* 0 represents orthogonality which means no relation
* 1 represents similar
"""



from gensim.models import Word2Vec

sentences = [["kind","human","compassion"]]

model = Word2Vec(sentences,vector_size = 100,min_count = 1)

embeddings = model.wv

print("Vector representation of 'kind' :")
print(embeddings['kind'])

print("\n Similarity b/w 'kind' and 'human'", model.wv.similarity('kind','human'))

import numpy as np
from sentence_transformers import SentenceTransformer

# Function to encode text with error handling
def encode_text(text):
    try:
        if isinstance(text, str) and text.strip():  # Check for empty/non-string values
            return model.encode(text)
        else:
            return np.nan  # Replace empty/invalid values with NaN
    except Exception as e:
        print(f"Error encoding: '{text}': {e}")
        return np.nan

# Encode text and numbers, handling potential errors
text_embeddings = X.loc[:, "text_clean"].apply(encode_text)
num_embeddings = X.loc[:, "text_len"].apply(lambda x: encode_text(str(x))) # Convert 'text_len' to string

# Check for NaN values (which indicate encoding errors)
print("Number of NaN values in text embeddings:", text_embeddings.isnull().sum())
print("Number of NaN values in number embeddings:", num_embeddings.isnull().sum())

#!pip install sentence_transformers
import numpy as np
from sentence_transformers import SentenceTransformer

# Function to encode text with error handling
def encode_text(text):
    try:
        if isinstance(text, str) and text.strip():  # Check for empty/non-string values
            return model.encode(text)
        else:
            return np.nan  # Replace empty/invalid values with NaN
    except Exception as e:
        print(f"Error encoding: '{text}': {e}")
        return np.nan

# Encode text and numbers, handling potential errors
text_embeddings = X.loc[:, "text_clean"].apply(encode_text)
num_embeddings = X.loc[:, "text_len"].apply(lambda x: encode_text(str(x))) # Convert 'text_len' to string

# Check for NaN values (which indicate encoding errors)
print("Number of NaN values in text embeddings:", text_embeddings.isnull().sum())
print("Number of NaN values in number embeddings:", num_embeddings.isnull().sum())

from sklearn.feature_extraction.text import CountVectorizer

# Create a CountVectorizer object with the desired parameters
countvector = CountVectorizer(binary=True, min_df=5, stop_words="english")

# Fit and transform the text data using the CountVectorizer
converted = countvector.fit_transform(canva.text_clean)

# Convert the sparse matrix to a dense matrix for printing
converted_dense = converted.todense()

# Print the shape of the converted matrix
print(converted_dense.shape)

# Get the feature names from the CountVectorizer
feature_names = countvector.get_feature_names_out()

# Print the number of feature names
print(len(feature_names))